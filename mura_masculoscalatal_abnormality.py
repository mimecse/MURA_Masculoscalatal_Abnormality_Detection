# -*- coding: utf-8 -*-
"""MURA_Masculoscalatal_Abnormality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-AV1lZPfU9WkkEASjojYY0pzIhYp8YXJ
"""

from google.colab import drive
drive.mount('/content/drive/')

from zipfile import ZipFile

file_name="/content/drive/MyDrive/Project/MURA-v1.1.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')

import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical, plot_model
import tensorflow as tf
from keras import backend as K
from keras.layers import Dense, SimpleRNN, GRU, Input, Reshape
from keras.models import Model, load_model
from keras import optimizers
from keras.callbacks import History, Callback, ModelCheckpoint, LearningRateScheduler
from sklearn.metrics import confusion_matrix, accuracy_score

class_to_int = {'XR_SHOULDER':0, 'XR_HUMERUS':1, 'XR_ELBOW':2, 'XR_FOREARM':3, 'XR_WRIST':4, 'XR_HAND':5, 'XR_FINGER':6}

train_paths = open('/content/MURA-v1.1/train_image_paths.csv', 'r').readlines()
train_labels = open('/content/train_image_class.csv', 'w')
for path in train_paths:
    train_labels.write(str(class_to_int[path.split('/')[2]])+'\n')

train_labels.close()

valid_paths = open('/content/MURA-v1.1/valid_image_paths.csv', 'r').readlines()
valid_labels = open('/content/valid_image_class.csv', 'w')
for path in valid_paths:
    valid_labels.write(str(class_to_int[path.split('/')[2]])+'\n')

valid_labels.close()

study_to_int = {'positive':1, 'negative':0}

train_paths = open('/content/MURA-v1.1/train_image_paths.csv', 'r').readlines()
train_labels = open('/content/train_labeled_studies1.csv', 'w')
for path in train_paths:
    train_labels.write(str(study_to_int[path.split('/')[4].split('_')[1]])+'\n')

train_labels.close()

valid_paths = open('/content/MURA-v1.1/valid_image_paths.csv', 'r').readlines()
valid_labels = open('/content/valid_labeled_studies1.csv', 'w')
for path in valid_paths:
    valid_labels.write(str(study_to_int[path.split('/')[4].split('_')[1]])+'\n')

valid_labels.close()

from random import randint
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from keras import backend as K
from keras.layers import Dense, GlobalAveragePooling2D
from keras import optimizers
from keras.applications.densenet import DenseNet121
from keras.models import Model, load_model
from keras.callbacks import History, Callback, ModelCheckpoint, LearningRateScheduler

# x-ray class to int encoding and vice versa
class_to_int = {'XR_SHOULDER':0, 'XR_HUMERUS':1, 'XR_ELBOW':2, 'XR_FOREARM':3, 'XR_WRIST':4, 'XR_HAND':5, 'XR_FINGER':6}
int_to_class = ['XR_SHOULDER', 'XR_HUMERUS', 'XR_ELBOW', 'XR_FOREARM', 'XR_WRIST', 'XR_HAND', 'XR_FINGER']
NUM_CLASS = 7

# image size
IMG_W = 320
IMG_H = 320
IMG_C = 3

# train image paths and their int_class
train_img_paths = np.array(open('/content/MURA-v1.1/train_image_paths.csv', 'r').readlines())
train_img_class = np.array(open('/content/train_image_class.csv', 'r').readlines()).astype(int)

# validation image paths and their int_class
valid_img_paths = np.array(open('/content/MURA-v1.1/valid_image_paths.csv', 'r').readlines())
valid_img_class = np.array(open('/content/valid_image_class.csv', 'r').readlines()).astype(int)

def densenet_model():
    # creating the base pre-trained model
    base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(IMG_H, IMG_W, IMG_C))

    # add a global spatial average pooling layer
    X = base_model.output
    X = GlobalAveragePooling2D()(X)
    X = Dense(64, activation='relu')(X)
    # add a logistic layer
    Y = Dense(NUM_CLASS, activation='softmax')(X)

    # defining the model
    model = Model(inputs=base_model.input, outputs=Y)
    return model

model = densenet_model()

# for setting which layers to be trained
for layer in model.layers:
    layer.trainable = True

model.summary()

BATCH_SIZE = 32

def lr_scheduler(epoch):
    #lrate = 0.001
    lrate = 0.0001
    return lrate*(0.1**(int(epoch/10)))

import tensorflow as tf
#INIT_LR = 0.003
INIT_LR = 0.0003
adam = tf.optimizers.Adam(INIT_LR)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

# image size
IMG_W = 320
IMG_H = 320
IMG_C = 3

def train_batch_img_paths_class_generator(img_paths, img_class, batch_size):
    # generating batches as many times requested
    while True:
        cnt = len(img_paths)
        permute = np.random.permutation(cnt).astype(int)
        permuted_img_paths = img_paths[permute]
        permuted_img_class = img_class[permute]
        i = 0
        while i < cnt:
            if (i + batch_size) <= cnt:
                batch_img_paths = permuted_img_paths[i:i + batch_size]
                batch_img_class = permuted_img_class[i:i + batch_size]
                i += batch_size
                yield batch_img_paths, batch_img_class
            else:
                batch_img_paths = permuted_img_paths[i:]
                batch_img_class = permuted_img_class[i:]
                i += batch_size
                yield batch_img_paths, batch_img_class

def valid_batch_img_paths_class_generator(img_paths, img_class, batch_size):
    # generating batches as many times requested
    while True:
        cnt = len(img_paths)
        permute = np.random.permutation(cnt).astype(int)
        permuted_img_paths = img_paths[permute]
        permuted_img_class = img_class[permute]
        i = 0
        while i < cnt:
            if (i + batch_size) <= cnt:
                batch_img_paths = permuted_img_paths[i:i + batch_size]
                batch_img_class = permuted_img_class[i:i + batch_size]
                i += batch_size
                yield batch_img_paths, batch_img_class
            else:
                batch_img_paths = permuted_img_paths[i:]
                batch_img_class = permuted_img_class[i:]
                i += batch_size
                yield batch_img_paths, batch_img_class

def img_preprocessing(img, flip=True):
    img = cv2.resize(img, (IMG_H, IMG_W), interpolation = cv2.INTER_AREA) # resize to 320 X 320
    img = (img - np.mean(img))/np.std(img) # normalization

    if randint(0, 1) and flip:
        img = cv2.flip(img, 1) # horizontal flip
#     if randint(0, 1) and flip:
#         img = cv2.flip(img, 0) # vertical flip

    #if randint(0,1) :
     #     angle = randint(-30,30)
      #    M = cv2.getRotationMatrix2D((IMG_H/2,IMG_W/2),angle,1)
       #   #The third parameter: the size of the transformed image
        #  image = cv2.warpAffine(image,M,(IMG_H,IMG_W))

    return img


def train_generator(img_paths, img_class, batch_size, NUM_CLASS, flip=True):
    while True: # for generating batch as many times required for training the model
        for batch_img_paths, batch_img_class in train_batch_img_paths_class_generator(img_paths, img_class, batch_size):
            num_img = len(batch_img_paths)
            batch_x = np.zeros((num_img, IMG_H, IMG_W, IMG_C))
            for i in range(num_img):
                img = cv2.imread(batch_img_paths[i].strip())
                img = img_preprocessing(img, flip)
                batch_x[i] = img
            batch_y = to_categorical(batch_img_class, num_classes=NUM_CLASS)
            yield (batch_x, batch_y)

def validation_generator(img_paths, img_class, batch_size, NUM_CLASS, flip=True):
    while True: # for generating batch as many times required for training the model
        for batch_img_paths, batch_img_class in valid_batch_img_paths_class_generator(img_paths, img_class, batch_size):
            num_img = len(batch_img_paths)
            vbatch_x = np.zeros((num_img, IMG_H, IMG_W, IMG_C))
            for i in range(num_img):
                img = cv2.imread(batch_img_paths[i].strip())
                img = img_preprocessing(img, flip)
                vbatch_x[i] = img
            vbatch_y = to_categorical(batch_img_class, num_classes=NUM_CLASS)
            yield (vbatch_x, vbatch_y)

# fit model
history = History()
model.fit_generator(train_generator(train_img_paths, train_img_class, BATCH_SIZE, NUM_CLASS),
                    steps_per_epoch = len(train_img_class)/BATCH_SIZE,
                    epochs = 20,
                    validation_data = validation_generator(valid_img_paths, valid_img_class, BATCH_SIZE, NUM_CLASS),
                    validation_steps = 100,
                    callbacks = [LearningRateScheduler(lr_scheduler)])

def predic_batch_img_paths_generator(img_paths, batch_size):
    cnt = len(img_paths)
    i = 0
    while i < cnt:
        if (i + batch_size) <= cnt:
            batch_img_paths = img_paths[i:i + batch_size]
            i += batch_size
            yield batch_img_paths
        else:
            batch_img_paths = img_paths[i:]
            i += batch_size
            yield batch_img_paths

def prediction_generator(img_paths, batch_size):
    while True: # for generating batch as many times required for training the model
        for batch_img_paths in predic_batch_img_paths_generator(img_paths, batch_size):
            num_img = len(batch_img_paths)
            pbatch_x = np.zeros((num_img, IMG_H, IMG_W, IMG_C))
            for i in range(num_img):
                img = cv2.imread(batch_img_paths[i].strip())
                img = img_preprocessing(img, False)
                pbatch_x[i] = img
            yield pbatch_x

from collections import Counter
cnt = Counter()
for c in valid_img_class:
    cnt[c]+=1
print(cnt)

predict_img_paths = []
predict_img_class = []
cnt = Counter()
for i in range(len(valid_img_paths)):
    if cnt[valid_img_class[i]]<288:
        predict_img_paths.append(valid_img_paths[i])
        predict_img_class.append(valid_img_class[i])
    cnt[valid_img_class[i]]+=1

predict_img_paths = np.array(predict_img_paths)
predict_img_class = np.array(predict_img_class)

pred_y = model.predict_generator(prediction_generator(predict_img_paths, BATCH_SIZE), steps=len(predict_img_paths)/BATCH_SIZE)

pred_y_class = np.argmax(pred_y, axis=1)
print(pred_y_class.shape)

print(len(predict_img_paths))

print(history.history.keys())

# confusion matrix and accuracy
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score
plt.figure(figsize=(7, 7))
plt.title('Confusion matrix', fontsize=16)
sns.heatmap(confusion_matrix(predict_img_class, pred_y_class), annot=True)
plt.xticks(np.arange(NUM_CLASS), int_to_class, rotation=45, fontsize=12)
plt.yticks(np.arange(NUM_CLASS), int_to_class, rotation=0, fontsize=12)
plt.xlabel('prediction', fontsize = 12)
plt.ylabel('original', fontsize = 12)
plt.show()
print("Test accuracy:", accuracy_score(predict_img_class, pred_y_class)*100)

"""Kappa score"""

from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score
cohen_kappa_score(predict_img_class, pred_y_class)

from sklearn.metrics import classification_report,precision_score,recall_score,accuracy_score,f1_score,cohen_kappa_score, roc_auc_score

print("Test accuracy:", accuracy_score(predict_img_class, pred_y_class))
print("Recall : ",recall_score(predict_img_class, pred_y_class, average='macro'))
print("Precision : ", precision_score(predict_img_class, pred_y_class, average='macro'))
print("F1 score: ",f1_score(predict_img_class, pred_y_class , average='macro'))
print("Cohen_kappa_score: ",cohen_kappa_score(predict_img_class, pred_y_class))

"""Classification report"""

print()
print('Classification Report')
print(classification_report(predict_img_class, pred_y_class, target_names=["0","1","2","3","4","5","6"]))